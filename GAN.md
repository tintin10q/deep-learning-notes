# Generative Adversial network

The idea of a GAN is to decide if an image is real or generated by a [Generative network](Generative%20network.md). The loss functions of these networks are combined. 

![Gan](Pasted%20image%2020221012140929.png)

![Generate a fake image](Pasted%20image%2020221012140946.png)

Discriminator:  
LD = Ex∼data[− log D(x)] + Ez∼noise[− log(1 − D(G(z)))],  

D "wants" D(x ) = 1, D(G(z)) = 0.  

Generator:  
LG = −LD = Ez[log(1 − D(G(z)))] + const,  
G "wants" D(G(z)) = 1

This is cross entropy loss. The generator loss is bound to how much it predicts correctly. The discriminator has a loss function based on how much the generator is wrong. 

You could say generator loss is 1-"loss of discriminator".

In the end you only want one of the two models so you choose the maximise the loss of one and minimise the loss of the other. 

In practice you want to improve both so you alternate maximising each one. 

Things can go wrong with this method

> If the discriminator is very good then the generator can never learn

> What happens if G always produces some x $\in$ data. Then you get mode collapse. 

These things mean that if you update one model to much than the learning stops